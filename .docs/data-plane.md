# :abacus: `Data Plane` 

:round_pushpin: Joining a node (to the cluster)

On control plane initialization, a :tickets: token was given with the `kubeadm` command, retrieve this command.

It should look like the following:

```
kubeadm join rukbat.sagittarius.gasy.africa:6443 \
                      --token jlqqn9.8nb1zcdjgwbn308k  \
                      --discovery-token-ca-cert-hash sha256:12232fe0c49fca1155c76364da335e26b72f95686af2dba8e1157f75a6c7f157 
```
> Returns:
```yaml
[preflight] Running pre-flight checks
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.

```

:bulb: Do not forget to add `sudo` 

# [:back:](../README.md#abacus-data-plane)


## :cl: Token Management :tickets:

If you lost the :tickets: token, or the token has expired

:round_pushpin: On the :control_knobs: control plane,

- [ ] Display list of :tickets: Tokens

Observe the `TTL` (Time To Live) field, it indicates the time remaining before the token expires with a lifespan of 24 hours

```
kubeadm token list
```
```yaml
TOKEN                     TTL         EXPIRES                USAGES                   DESCRIPTION                                                EXTRA GROUPS
jlqqn9.8nb1zcdjgwbn308k   1h          2023-04-16T11:02:25Z   authentication,signing   The default bootstrap token generated by 'kubeadm init'.   system:bootstrappers:kubeadm:default-node-token
```

:star: If the token has not expired:

:control knobs: On the control plane, retrieve the following information

- [ ] The :hash: of the `SHA256` certificate

```yaml
openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt \
    | openssl rsa -pubin -outform der 2>/dev/null \
    | openssl dgst -sha256 -hex \
    | sed 's/^.* //'
```
> 2ac9eca4748cb94db31f3f271bf49d3149287bb664050a82bf8dc97bf673daa0

- [ ] The expired :tickets: token returns nothing

```
kubeadm token list
```

:abacus: On the data plane

- [ ] Give the recovered values to the following environment variables: (for example)

```powershell
CTL_PLANE="rukbat.sagittarius.gasy.africa" ; \
TOKEN="rrmied.zty2u72rw09jm8da" ; \
CA_CERT_HASH="2ac9eca4748cb94db31f3f271bf49d3149287bb664050a82bf8dc97bf673daa0"
```

- [ ] Run the command to join the cluster

```powershell
sudo kubeadm join ${CTL_PLANE}:6443 --token ${TOKEN} --discovery-token-ca-cert-hash sha256:${CA_CERT_HASH}
```

:round_pushpin: If the tokens have all expired, regenerate a token with the commands below

:control_knobs: On the control plane 

```powershell
kubeadm token create --print-join-command
```
> Returns
```
W0325 19:10:34.388061   53964 configset.go:202] WARNING: kubeadm cannot validate component configs for API groups [kubelet.config.k8s.io kubeproxy.config.k8s.io]
kubeadm join betelgeuse.orion.gasy.africa:6443 --token fu6544.o36km11eg95slej1     --discovery-token-ca-cert-hash sha256:348cf90011e40088944a5f5cfe3279c04a0dfb24f56ba21209e61fdc15af3645
```

:abacus: On the data plane

```bash
$ sudo kubeadm join rukbat.sagittarius.gasy.africa:6443 --token fu6544.o36km11eg95slej1     --discovery-token-ca-cert-hash sha256:348cf90011e40088944a5f5cfe3279c04a0dfb24f56ba21209e61fdc15af3645 
```

## :x: Troubleshooting

:abacus: On the data plane

- [ ] [Container runtime network not ready: cni config uninitialized](https://stackoverflow.com/questions/49112336/container-runtime-network-not-ready-cni-config-uninitialized)

* Disable the security service [`apparmor`]([https://www.apparmor.net/](https://kubernetes.io/docs/tutorials/security/apparmor/))

```
systemctl stop apparmor ; systemctl disable apparmor ; systemctl restart containerd.service
```

:control_knobs: On the control plane 

List the nodes and get the `<node-name>` you want to drain (removed from the cluster)

```
kubectl get nodes
```

1) First, drain the node

```
kubectl drain <nom-du-neoud>
```

You can ignore `daemonsets` and local data in the machine

```
kubectl drain <node-name> --ignore-daemonsets --delete-local-data
```
  
2) Finally, remove the node

```
kubectl delete node <node-name>
```
  
#### :recycle: Reset the node 

- [ ] Pre-fligth `/etc/kubernetes/kubelet.conf already exists`
    
```
sudo kubeadm join ${CTL_PLANE}:6443 --token ${TOKEN} --discovery-token-ca-cert-hash sha256:${CA_CERT_HASH}
```
> Returns:
```yaml
[preflight] Running pre-flight checks
error execution phase preflight: [preflight] Some fatal errors occurred:
        [ERROR FileAvailable--etc-kubernetes-kubelet.conf]: /etc/kubernetes/kubelet.conf already exists
        [ERROR FileAvailable--etc-kubernetes-pki-ca.crt]: /etc/kubernetes/pki/ca.crt already exists
[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`
To see the stack trace of this error execute with --v=5 or higher
```

- [ ] Reset
    
```
 sudo kubeadm reset
```
> Returns:
```
[reset] WARNING: Changes made to this host by 'kubeadm init' or 'kubeadm join' will be reverted.
[reset] Are you sure you want to proceed? [y/N]: y
[preflight] Running pre-flight checks
W0325 18:59:42.226203 3662407 removeetcdmember.go:79] [reset] No kubeadm config, using etcd pod spec to get data directory
[reset] No etcd config found. Assuming external etcd
[reset] Please, manually reset etcd to prevent further issues
[reset] Stopping the kubelet service
[reset] Unmounting mounted directories in "/var/lib/kubelet"
[reset] Deleting contents of config directories: [/etc/kubernetes/manifests /etc/kubernetes/pki]
[reset] Deleting files: [/etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/bootstrap-kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf]
[reset] Deleting contents of stateful directories: [/var/lib/kubelet /var/lib/dockershim /var/run/kubernetes /var/lib/cni]

The reset process does not clean CNI configuration. To do so, you must remove /etc/cni/net.d

The reset process does not reset or clean up iptables rules or IPVS tables.
If you wish to reset iptables, you must do so manually by using the "iptables" command.

If your cluster was setup to utilize IPVS, run ipvsadm --clear (or similar)
to reset your system's IPVS tables.

The reset process does not clean your kubeconfig files and you must remove them manually.
Please, check the contents of the $HOME/.kube/config file.
  
```
  
- [ ] Clean up leftover files

```
sudo rm -rf /etc/cni $HOME/.kube/config
```
  
:interrobang: Unhealthy :droplet: Kubelet
    
```
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...
[kubelet-check] Initial timeout of 40s passed.

Unfortunately, an error has occurred:
        timed out waiting for the condition

This error is likely caused by:
        - The kubelet is not running
        - The kubelet is unhealthy due to a misconfiguration of the node in some way (required cgroups disabled)

If you are on a systemd-powered system, you can try to troubleshoot the error with the following commands:
        - 'systemctl status kubelet'
        - 'journalctl -xeu kubelet'
error execution phase kubelet-start: timed out waiting for the condition
To see the stack trace of this error execute with --v=5 or higher
```
    
# References

- [ ] [How do I find the join command for kubeadm on the master?](https://stackoverflow.com/questions/51126164/how-do-i-find-the-join-command-for-kubeadm-on-the-master)
- [ ] [How can I execute local script on remote machine and include arguments?](https://unix.stackexchange.com/questions/87405/how-can-i-execute-local-script-on-remote-machine-and-include-arguments)
- [ ] [Using local-exec and remote-exec provisioners with terraform](https://sdorsett.github.io/post/2018-12-26-using-local-exec-and-remote-exec-provisioners-with-terraform/)
- [ ] [How to gracefully remove a node from Kubernetes?](https://stackoverflow.com/questions/35757620/how-to-gracefully-remove-a-node-from-kubernetes)

